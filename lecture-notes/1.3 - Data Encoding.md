# 1.3 Data Encoding

Recommended reading:

- Hall & Slonka, pp. 13-18

## Bits and Bytes

Numbers and number systems are abstract. Computers utilize binary numbers to store and manipulate data, but they operate in the real world. This means there is a physical limit on how big the numbers can be.

Each digit in a binary number is referred to as a _bit_. A set of eight bits is called a _byte_. Bytes are the smallest unit of data a computer stores, fetches, and operates on. Bytes are stored in computer memory and are assigned a memory address. Individual bits do not have an address, but each byte does.

We may want to reference individual bits within a byte in certain circumstances (e.g., writing documentation). A common way to do this is to label the bits starting with `0` for the least-significant bit and incrementing by one for each successive bit. In the example below, the value of the bit at index `4` is `1`.

```
Bit index:  7 6 5 4 3 2 1 0
Bit value:  0 0 1 1 0 0 0 1
```

The smallest unsigned integer (positive whole number) you can store in a byte is `0`; the largest is `255`. You can determine the available range of values given a number of bits using the formula 2<sup>n</sup>, where _n_ is the number of bits.

- 2<sup>8</sup> = 256 possible values [0-255]

### Data Sizes and Words

A byte is the smallest data size the CPU works with, but a single byte isn't always enough. CPUs have grown over the years to support different sizes of data.

A byte is universally recognized as eight bits across all modern computers, but different CPU architectures have different names for bigger data sizes. The table below lists data sizes we'll be using in this course, as defined by the x86-64 instruction set.[^x86-data-sizes]

| Name | Bytes | Bits | Unsigned range |
| :- | :- | :- | :- |
| byte | 1 byte | 8 bits | 0 to 255 |
| word | 2 bytes | 16 bits | 0 to 65,535 |
| doubleword | 4 bytes | 32 bits | 0 to 4,294,967,295 |
| quadword | 8 bytes | 64 bits | 0 to over 18 quintillion |

You may also see the term _word_ in the context of computer architecture and CPU instructions sets. This usually refers to the largest data size the CPU supports, or the "natural" data size of the CPU.[^word-data-size] For example, if you have a 64-bit CPU, you could say the CPU has a word size of 64 bits.

This can get confusing when dealing with x86-64 and the Windows operating system, both of which refer to a _word_ as 16 bits. This is for historical purposes to maintain backwards compatibility. When the x86 instruction set expanded from 16 bits to 32 bits, they kept a _word_ equal to 16 bits and started calling 32 bits a _doubleword_ or `DWORD`. Since x86-64 is the primary instruction set we'll be using in this course, so we'll use these names when referring to different data sizes.

[^x86-data-sizes]: Hall & Slonka, p. 9.

[^word-data-size]: https://en.wikipedia.org/wiki/Word_(computer_architecture)

## Signed Integers

So far we've only looked at positive, whole numbers (`0`, `1`, `2`, etc.). These are classified as _unsigned integers_. But what if we need to work with negative numbers?

In the abstract world, we can put a negative sign in front of the number. For example, the decimal number `-5` is equal to `-101` in binary. But in a real computer, we don't have that luxury. We need a way of encoding a negative number into binary digits.

Here are three methods for encoding _signed integers_:

- _Sign-magnitude_: leading bit indicates positive (`0`) or negative (`1`)
- _One's complement_: flip the bits
- _Two's complement_: flip the bits and add one

A good introduction to each method can be found on page 17 of the textbook.[^textbook-3bit-example] The videos at https://lowlevelcoder.com are particularly useful.

[^textbook-3bit-example]: Hall & Slonka, p. 17.

**It's very important when dealing with signed integers to know the data size you are dealing with.** 

For example, convert the value (-8)<sub>10</sub> into 8-bit, two's complement form.

```
 8 = 00001000
-8 = 11110111 + 1 = 11111000 (twoâ€™s complement)
```

What if you forget to use the full eight bits?

```
 8 = 01000
-8 = 10111 + 1 = 11000 (twoâ€™s complement) - WRONG
```

When dealing with an 8-bit value, `11000` would be padded with leading zeros to `00011000`, which is not the correct answer for the question.

## Characters

We need to be able to store and manipulate text on a computer. Since a computer operates using binary, we need a way to encode text characters into numbers. We'll look at ASCII and Unicode and how they achieve this.

[This article](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/) by Joel Spolsky is very informative on text encoding, and is worth reading.

### ASCII

One method of character encoding is [ASCII](https://en.wikipedia.org/wiki/ASCII). ASCII defines a standard mapping of characters into numbers. There are 128 possible characters, which means each character uses seven bits. Check out https://www.ascii-code.com for a table of ASCII characters.

ASCII characters use seven bits, but a byte is eight bits. So there is an extra bit that unlocks another 128 characters. There are many different mapping schemes for determining what these additional characters are. Understanding all these mapping schemes and the history behind them gets confusing quick: different computer manufacturers, software developers, and operating systems had many different variations. Check out the Wikipedia page for [code page](https://en.wikipedia.org/wiki/Code_page) to get a sampling.

### Unicode

Unicode is the current go-to standard for text encoding.

Unicode defines a set of _characters_. Each character is an abstract concept. The uppercase "A" is different from lowercase "a".

Each character maps to a _code point_. For example, the capital letter "A" maps to the code point [U+0041](https://unicode-table.com/en/0041/). All the mappings of characters to code points make up the Unicode _character set_.

The [Unicode consortium](https://home.unicode.org/) is in charge of creating and updating the character set. They figure out all the nuances of different languages and figure out what characters map to which code point. The character set includes a vast array of languages, symbols, and, of course, [emojis](https://unicode-table.com/en/blocks/emoticons/). (And [domino tiles](https://unicode-table.com/en/blocks/domino-tiles/) and [playing cards](https://unicode-table.com/en/blocks/playing-cards/).)

Notice that code points are still abstract concepts. How they get encoded into bits that are stored in memory is handled by the _character encoding_. There are various [encoding schemes](https://en.wikipedia.org/wiki/UTF-8#Encoding) for Unicode. UTF-8 is the most popular on the Web.[^utf8-popular]

[^utf8-popular]: https://w3techs.com/technologies/cross/character_encoding/ranking

If you look at the Unicode character set, you'll notice there are far more than 256 characters - which means a single byte per character won't cut it. UTF-8 works as a variable length encoding scheme. It defines a mapping of Unicode code points into a series of one to four bytes.

The beauty of UTF-8 is the first 128 code points are identical to ASCII. These code points use only one byte, which means UTF-8 is backwards compatible with ASCII. Any code points beyond that require more bytes.

The following table is copied from [Wikipedia](https://en.wikipedia.org/wiki/UTF-8#Encoding) and shows how each range of code points map into bytes:

| First code point | Last code point | Byte 1 | Byte 2 | Byte 3 | Byte 4 |
| -: | -: | - | - | - | - |
| U+0000 | U+007F | `0xxxxxxx` | | | |
| U+0080 | U+07FF | `110xxxxx` | `10xxxxxx` | | |
| U+0800 | U+FFFF | `1110xxxx` | `10xxxxxx` | `10xxxxxx` | |
| U+10000 | U+10FFFF | `11110xxx` | `10xxxxxx` | `10xxxxxx` | `10xxxxxx` |

Browse through https://symbl.cc/en/unicode/table to see all the different Unicode characters, their code points, and different encoded values. For example:

- [A - Latin Capital Letter A](https://symbl.cc/en/0041/)
- [ðŸš€ - Rocket Emoji](https://symbl.cc/en/1F680/)

## Useful Links

Rollover/underflow/overflow:
- [xkcd: Can't Sleep](https://xkcd.com/571/)
- [Overflow Error - TV Tropes](https://tvtropes.org/pmwiki/pmwiki.php/Main/OverflowError)
- [Sonic level warp underflow](https://youtu.be/O_6a-BJC0O8?t=66)

Signed integers:
- [Textbook Videos](https://lowlevelcoder.com)
- [Ben Eater Two's Complement Video](https://www.youtube.com/watch?v=4qH4unVtJkE)

Other signed integer encoding methods:
- [Protocol Buffers ZigZag](https://developers.google.com/protocol-buffers/docs/encoding?csw=1#signed-ints)

Character encoding:
- [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)
- [Code Page Identifiers - Win32 apps | Microsoft Learn](https://learn.microsoft.com/en-us/windows/win32/intl/code-page-identifiers)
